- 
    name: 2014_flexISP
    url: http://www.cs.ubc.ca/labs/imager/tr/2014/FlexISP/FlexISP_Heide2014_lowres.pdf
    notes:
        - end-to-end image processing that enforce image priors as proximal operators and use ADMM/primal-dual for optimization
        - end-to-end reduces error introduced in each steps of image processing, as each stage is not independent
        - applied to demosaicing, denoising, deconvolution, and a variety of reconstruction tasks
    evaluate:
        - recontruction based, no need for large datasets
        - classical regularizers that have proven to work well
        - choice of prior is shown to influence performance, so the choice of prior is important but ad hoc
        - no principled ways to pick solver parameters, i.e. the weight of regularizers
        - nonconvexity of the regularizers makes the optimization not guaranteed to converge to global optimum
        

-
    name: 2016_sequential_energy_minimization
    url: https://pure.tugraz.at/ws/portalfiles/portal/3625282/0004.pdf
    notes:
        - demosacing+denoising as image restoration problem
        - method learngs efficient regularization by a variational energy minimization 
        - demosaicing performed in linear RGB space
        - adaptable to different CFA pattern
        - regularization is learnt from data
        - noise level is more realistic, i.e. Poisson + Gaussian
        - data fidelity term and image priors are learnt from data
    evaluate:
        - 200 training images, which seems reasonable
    compare_to:
        - 2014_flexISP
        - 2016_sequential_energy_minimization
    results:
        msr_linear:
            - 40.00
            - 40.92
        msr_linear_noisy:
            - 38.28
            - 38.93
        

-
    name: 2016_deepjoint
    url: https://groups.csail.mit.edu/graphics/demosaicnet/data/demosaicnet_slides.pdf
    slides: https://groups.csail.mit.edu/graphics/demosaicnet/data/demosaicnet_slides.pdf
    notes:
        - data-driven (instead of hand-crafted priors) approach to demosaicing and denoising using deep neural nets
        - goal is to reduce computation speed and reduce artifacts
        - generated millions of sRGB images, mosaicked, and injected noise.
        - came up with new metrics for identifying hard patches
    evaluate:
        - requires 2.5M images, not really feasible
    compare_to:
        - 2014_flexISP
        - 2016_deepjoint
    results:
        kodak:
            - 40.0
            - 41.2
        mcm:
            - 38.6
            - 39.5
        msr_linear:
            - 40.0
            - 42.7
            
- 
    name: 2017_jointadmm
    url: https://www.researchgate.net/profile/Hanlin_Tan/publication/317058420_Joint_demosaicing_and_denoising_of_noisy_bayer_images_with_ADMM/links/59479f95a6fdccfa5949fc82/Joint-demosaicing-and-denoising-of-noisy-bayer-images-with-ADMM.pdf?origin=publication_detail
    notes:
        - unified objective function with hidden priors, optimized wth ADMM for demosaicing noisy bayer input
        - included 4 prior terms, i.e. smoothness TV, denoising CBM3D, cross-channel, interpolation-based priors
        - results showed ADMM based more robust to noise
    evaluate:
        - purely optimization based
        - however regularizers are pretty ad hoc
    compare_to:
        - 2014_flexISP
        - 2016_deepjoint
        - 2017_jointadmm
    results:
        kodak:
            - 34.98
            - 33.88
            - 31.63
        mcm:
            - 35.18
            - 32.49
            - 32.66
        kodak_sigma15:
            - 26.67
            - 30.40
            - 30.16
        mcm_sigma15:
            - 26.55
            - 29.89
            - 30.50
            
- 
    name: 2017_learned_proximal_operators
    url: http://openaccess.thecvf.com/content_ICCV_2017/papers/Meinhardt_Learning_Proximal_Operators_ICCV_2017_paper.pdf
    notes:
        - replace regularizer in energy minimization methods (primal-dual hybrid gradient PDHG) with a denoising neural network
            - residual denoising network DnCNN (https://arxiv.org/pdf/1608.03981.pdf)
        - reduce problem-specific training (i.e. different images, different noise levels)
        - related work section detailed some theory, proofs on convergence of custom proximal algorithms
        - RED is motivated by the observation that proximal operator is equivalent to image denoising
        - method implemented in a DSL for proximal methods in processing images (http://people.csail.mit.edu/jrk/proximal.pdf)
    evaluate:
        - DnCNN trained with 400 images of size 180x180, larger dataset negligible performance increase
        - seem to claim that using neuralnets for regularizers is better
    compare_to:
        - 2014_flexISP
        - 2016_deepjoint
        - 2017_learned_proximal_operators
    results:
        mcm:
            - 36.12
            - 39.5
            - 37.12
    points:
        - the mehtod is general for any inverse problem, not just demosaicing, so performed worse than demosaicnet, but is better than flexISP

         
-
    name: 2018_jointgan_perceptual
    url: https://arxiv.org/pdf/1802.04723.pdf
    notes:
        - used discriminator instead of PSNR/SSIM to evaluate perceptual quality of the reconstructed image
        - jointly optimize for demosaic and denoise
    evaluate:
        - 1400 high quality color photo, into 100x100 patches, data-augmentation x8 -> 320000 iamges
        - maybe manageable
    compare_to:
        - 2014_flexISP
        - 2016_sequential_energy_minimization
        - 2016_deepjoint
        - 2017_jointadmm
        - 2018_jointgan_perceptual
    results:
        kodak_sigma20:
            - 25.15
            - 23.00
            - 29.17
            - 29.26
            - 30.70
            - 30.74
        mcm_sigma20:
            - 25.01
            - 22.99
            - 28.79
            - 29.31
            - 30.73
            - 30.77
            
- 
    name: 2018_deepdemosaicking
    url: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8283772
    notes:
        - initial demosaic combined with demosaic refinement reduces color artifacts using deep residual estimation 
    evaluate:
        - 838 high res image from several sources -> 100000 64x64 patches
    compare_to:
        - 2016_deepjoint
        - 2018_deepdemosaicking
    results:
        kodak:
            - 41.45
            - 42.12
            
- 
    name: 2018_iterative_resdnet_joint
    previous_work: https://arxiv.org/pdf/1803.05215.pdf
    code: https://github.com/cig-skoltech/deep_demosaick
    url: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8668795
    notes:
        - iterative neural networks for efficient optimization (majorization minimization)
        - use resdnet to learn the regularizing term
        - the algorithm is equivalent to Inexact Proximal Gradient Descent
        - used more realistic affine noise model instead of gaussian noise model
            - http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.1943&rep=rep1&type=pdf
    evaluate:
        - compared to RED, P3, this method does not require setting of hyperparameters
        - resdnet pretrained on berkeley dataset (500 color images)
        - microsoft dataset, 500 natural images with panasonic dmc-lx3 ccd with bayer cfa
            - 200 training images
    compare_to:
        - 2014_flexISP
        - 2016_sequential_energy_minimization
        - 2016_deepjoint
        - 2018_deepdemosaicking
        - 2018_iterative_resdnet_joint
    results:
        kodak:
            - 40
            - 35.3
            - 41.2
            - 42
            - 42
        mcm:
            - 38.6
            - 30.8
            - 39.5
            - 39
            - 39.7
        msr_linear:
            - 40
            - 40.9
            - 42.7
            - 0
            - 42.8
        msr_linear_noisy:
            - 0
            - 38.8 
            - 38.6
            - 0
            - 40.1
    points:
        - this method trained on 200 linear noise free RGB images outperforms 2016_deepjoint
        
        
- 
    name: 2018_deepjoint_design
    url: https://inf.ufrgs.br/~bhenz/projects/joint_cfa_demosaicing/deep_joint_design_of_color_filter_arrays_and_demosaicing_pre-print.pdf
    notes:
        - uses autoencoder to encode input to `multispectral mosaic`, which is then decoded to full resolution image
        - finds mosaic pattern and demosaicing algorithm that minimizes color-reconstruction error
        - achieved state of arts result, while allowing for cfa mosaic design
    evaluation:
        - has some good background references on cfa design, maybe applicable in our case
        - a followup https://evanfletcher42.com/2018/09/23/arbitrary-learned-mosaic/
    compare_to:
        - 2016_deepjoint
        - 2018_deepjoint_design
    results:
        kodak:
            - 41.79
            - 41.86
        mcm:
            - 39.14
            - 39.51
        kodak_sigma20:
            - 30.00
            - 31.20
        mcm_sigma20:
            - 30.15
            - 30.87


-
    name: 2019_deepisp
    url: https://arxiv.org/pdf/1801.06724.pdf
    notes:
        - learns mapping from raw low-light mosaiced image to output image
            - low-level tasks (demosaicing+denoising)
            - high-level (color correction, image adjustment)
    evaluate:
        - masking patterns are differentiable weights inside encoder
        - training dataset too large to be feasible, 5 days of training time
    compare_to:
        - 2014_flexISP
        - 2016_sequential_energy_minimization
        - 2016_deepjoint
        - 2019_deepisp
    results:
        msr_linear:
            - 38.28
            - 38.93
            - 38.6
            - 39.31
        

- 
    name: 2019_multiframe_superres
    url: https://arxiv.org/pdf/1905.03277.pdf
    notes:
        - the method creates a complete RGB without demosaicing from a burst of CFA raw images 
        - the method uses natural hand tremor to acquire images with small offsets
        - the frames are aligned and merged to form a single image
        - solve demosaicing + superresolution as an image reconstruction problem
        - does not rely on cross-channel correlation
    evaluate:
        - hand tremor not feasible for c2b camera
    compare_to:
        - 2014_flexISP
        - 2016_deepjoint
        - 2019_multiframe_superres
    results:
        kodak:
            - 35.08
            - 39.67
            - 42.86
        mcm:
            - 35.15
            - 37.58
            - 41.26
    points:
        - the comparison is done with synthetic bursts, i.e. introduce offsets to 1 image